{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    " # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "          shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta = tf.placeholder(tf.float32)\n",
    "\n",
    "  # Variables.\n",
    "    weights = tf.Variable(\n",
    "      tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta * tf.nn.l2_loss(weights)\n",
    "\n",
    "  # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "  # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "      tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 22.294689\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 17.0%\n",
      "Minibatch loss at step 500: 2.289596\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 1000: 1.087309\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 1500: 0.565089\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 2000: 0.602535\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2500: 0.723652\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 3000: 0.718541\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 89.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: .003}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "betas = [pow(10, i) for i in np.arange(-5, 0, 0.1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "accuracies = []\n",
    "for beta_test in betas:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "        for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "        # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "            # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "            # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta: beta_test}\n",
    "            _, l, predictions = session.run(\n",
    "                  [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracies.append(accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAF4CAYAAADe9GoBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9kOFTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c513b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.semilogx(betas, accuracies)\n",
    "plt.title('Beta vs. accuracy - logistic')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_nn = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(layer1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_nn* (tf.nn.l2_loss(weights1)+tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(layer1_valid, weights2) + biases2)\n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 681.881409\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 28.5%\n",
      "Minibatch loss at step 500: 198.831894\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 1000: 116.618217\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 80.9%\n",
      "Minibatch loss at step 1500: 68.321312\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 83.0%\n",
      "Minibatch loss at step 2000: 41.195415\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 2500: 25.080158\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 3000: 15.485303\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.5%\n",
      "Test accuracy: 93.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_nn: .001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_nn = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(layer1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_nn* (tf.nn.l2_loss(weights1)+tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(layer1_valid, weights2) + biases2)\n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-515dcb194afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_nn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbeta_test\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             _, l, predictions = session.run(\n\u001b[0;32m---> 22\u001b[0;31m               [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "betas = [pow(10, i) for i in np.arange(-5, 0, 0.5)]\n",
    "accuracies = []\n",
    "num_steps = 3001\n",
    "\n",
    "for beta_test in betas:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.initialize_all_variables().run()\n",
    "\n",
    "        for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_nn: beta_test}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        accuracies.append(accuracy(test_prediction.eval(),test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhgAAAF4CAYAAAAWmIDXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSrUyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13c28ab50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.semilogx(betas[:len(accuracies)], accuracies)\n",
    "plt.title('Beta vs. accuracy - nn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "  \n",
    "  # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    logits = tf.matmul(layer1, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(layer1_valid, weights2) + biases2)\n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 310.360382\n",
      "Minibatch accuracy: 19.5%\n",
      "Validation accuracy: 28.4%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Minibatch loss at step 200: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 63.2%\n",
      "Test accuracy: 70.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 300\n",
    "num_batches = 6\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "overfits the minibatch, but doesn't do well on validation or testing data since there is no regularization and the number of training examples is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_nn = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    dropout = tf.nn.dropout(layer1,.5)\n",
    "    logits = tf.matmul(dropout, weights2) + biases2\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))  + beta_nn* (tf.nn.l2_loss(weights1)+tf.nn.l2_loss(weights2))\n",
    "  # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(layer1_valid, weights2) + biases2)\n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 767.227295\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 30.9%\n",
      "Minibatch loss at step 100: 285.337250\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.1%\n",
      "Minibatch loss at step 200: 259.943848\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 300: 233.708878\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.9%\n",
      "Test accuracy: 78.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 301\n",
    "num_batches = 6\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_nn: .001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 100 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does better with dropout, also added regularization for a slight improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_nodes_1 = 2**10\n",
    "hidden_nodes_2 = 2**8\n",
    "hidden_nodes_3 = 2**7\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    beta_nn = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  \n",
    "  # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, hidden_nodes_1],stddev=np.sqrt(1.0 / (50*image_size * image_size))))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_nodes_1]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes_1, hidden_nodes_2],stddev=np.sqrt(1.0 / hidden_nodes_1)))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_nodes_2]))\n",
    "    weights3 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes_2, hidden_nodes_3],stddev=np.sqrt(1.0 / hidden_nodes_2)))\n",
    "    biases3 = tf.Variable(tf.zeros([hidden_nodes_3]))\n",
    "    weights4 = tf.Variable(\n",
    "        tf.truncated_normal([hidden_nodes_3, num_labels],stddev=np.sqrt(4.0 / hidden_nodes_3)))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "    layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    drop1 = tf.nn.dropout(layer1, 0.5)\n",
    "    layer2 = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "    #drop2 = tf.nn.dropout(layer2, 0.5)\n",
    "    layer3 = tf.nn.relu(tf.matmul(layer2, weights3) + biases3)\n",
    "    \n",
    "    logits = tf.matmul(layer3, weights4) + biases4\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) + beta_nn* (tf.nn.l2_loss(weights1)+tf.nn.l2_loss(weights2)+tf.nn.l2_loss(weights3)+tf.nn.l2_loss(weights4))\n",
    "  \n",
    "  # Optimizer.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 4000,.65,staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    layer1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    layer2_valid = tf.nn.relu(tf.matmul(layer1_valid, weights2) + biases2)\n",
    "    layer3_valid = tf.nn.relu(tf.matmul(layer2_valid, weights3) + biases3)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(layer3_valid, weights4) + biases4)\n",
    "    layer1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    layer2_test = tf.nn.relu(tf.matmul(layer1_test, weights2) + biases2)\n",
    "    layer3_test = tf.nn.relu(tf.matmul(layer2_test, weights3) + biases3)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(layer3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.323068\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 23.4%\n",
      "Minibatch loss at step 1000: 0.586732\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 2000: 0.381879\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 3000: 0.481350\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 4000: 0.451648\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 5000: 0.501731\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "Minibatch loss at step 6000: 0.571493\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 7000: 0.469047\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 8000: 0.668314\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 9000: 0.453589\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 10000: 0.369306\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 11000: 0.350343\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 12000: 0.498139\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 13000: 0.420916\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 14000: 0.416945\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 15000: 0.339174\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 16000: 0.283633\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 17000: 0.267676\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 18000: 0.300540\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 19000: 0.263218\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 20000: 0.419257\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.5%\n",
      "Test accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_nn: .0001}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 1000 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "highest score without dropout is 96.3%. with dropout1 I got 96.4. added dropout2: 95.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
